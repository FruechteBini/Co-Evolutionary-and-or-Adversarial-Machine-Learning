This chapter compares the \textit{failure probability predictor} (AVF) (section \ref{avf}) and the \textit{Scenario Co-Evolution} (SCoE) (section \ref{coevolution}) testing approaches. The goal of each concept is to outperform standard evaluation approaches for reinforcement learning agents by finding a more efficient method of detecting rare, catastrophic failures. While the AVF uses \textit{Importance Sampling} to actively search for failures and estimate the failure risk, the SCoE uses \textit{Evolutionary Algorithms} to evolve a set of hard test scenarios to evaluate the agent. While both architectures outperform traditional random testing methods and solve the problem of having to test the agent on every scenario, they are fundamentally different.\\\\
Both algorithms co-evolve with an agent. Because the AVF looks for failures of this specific agent, it specializes on it. Meaning, the testing algorithm cannot be used to evaluate another, independently trained agent running on the same domain.\\
SCoE however does just that. The resulting test scenarios of the SCoE are hard to solve for any other agent. Once the SCoE returns a set of test scenarios, all agents running on the same environment can be evaluated with this exact set. SCoE is a more generic and simpler way to test agents on a specific domain.\\\\
A standard random reinforcement learning agent performs worse on SCoE generated scenarios than on random generated scenarios. It shows that SCoE generated scenarios discover more catastrophic failures than traditional generated scenarios. Hence, the SCoE testing process is more reliable. Still, we simply assume, that the agent can solve easy scenarios, if it is able to overcome hard cases. The agent gets better in all scenarios by training on hard scenarios, but still does not explicitly look for failures like the AVF.\\
As already described, the AVF actively searches for failures of the agent. In comparison to a standard \textit{Vanilla Monte Carlo} (VMC) estimator, the AVF minimizes the errors in risk estimation in a fraction of the time. For example, the VMC needs 12000 episodes in a driving domain whereas the AVF minimizes the errors in risk estimation after only 2000 episodes. Additionally, the error rate of the AVF is smaller after 2000 episodes, than the VMCs error rate ever is. Unfortunately, there is no measurement of the error rate for the SCoE. Because the AVF actively searches for failures while the SCoE "only" provides hard test cases, it might lead to the assumption, that the AVF is less error-prone.\\
Optimization through evolutionary algorithms tend to be slow for computationally intensive problems regarding the evaluation of each problem \cite{cruz}. Importance Sampling is often used to produce fast learning algorithms \cite{chen18, schaal04}. The underlying search algorithms lead to the supposition, that AVF (with Importance Sampling) might faster than SCoE (with Evolutionary Algorithms). Unfortunately, there is no direct comparison possible.\\
The SCoE was used to train and test an agent trying to transport items to workstations in a comprehensible multi-agent $7 \times 8$ grid \textit{smart factory domain}. The AVF was evaluated in a continuous \textit{driving} domain, where the agent was rewarded for forward progress without crashing and in a continuous \textit{humanoid} domain, where the agent was rewarded for standing without falling. It is not known, how the AVF performs in a multi-agent domain and vice versa. At this point, we can not know, which method is a better fit for a specific environment.\\\\
In summary, the SCoE is a more intuitive approach, can evaluate independent agents and is proven to work great in small grid domains. The AVF might be more reliable, reduces time of searching for failures, but specializes in one agent.