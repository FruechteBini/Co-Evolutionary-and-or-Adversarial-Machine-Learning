As mentioned before, a thorough testing process is necessary before integrating the learning system in a safety critical domain. When using a traditional random testing algorithm, usually the confidence that the failure rate of a policy is below $\epsilon$ requires at least $1/\epsilon$ testing episodes. This might be inefficient and time-intensive. For example in a autonomous driving domain. If there is a greater failure probability than $\epsilon = 10^{-8}$ per mile, meaning the car would crash at least once in 100 million miles, it would not be brought on the market. In order to achieve reasonable confidence, the manufacturer would need to test-drive the car for at least $10^{-8}$ miles, which may be prohibitively expensive \cite{uesato18}. Additionally, we don't exactly know how the environment will behave unless all possibilities are tried. For a huge variety of scenarios this process can get unreasonable. The agent can be trained and tested on random samples from the scenario space. However, this might cause the agent to specialize on the average scenario, which might provoke incidents. Hence, the agent is still not fully dependable \cite{gabor19}. To overcome these obstacles, a novel testing approach is needed.

\subsubsection{Intelligence Tests}
\label{intelligence}
Hern√°ndez-Orallo \& Dowe \cite{orallo10} introduced the \textit{anytime intelligence test}. This test adapts to the current intelligence of the system. When the systems' intelligence changes, the test adjusts itself. The basic idea is to initially test the agent on easy levels. When the agent succeeds it needs to overcome a harder environment. A set of environments of different complexity is required to evaluate the agent. These environments were created on a theoretical base (e.g. Solomonoff prediction) with a determined a priori complexity. This approach tests the intelligence of a system, but can not thoroughly test a learning system. The autonomous car, for example, still needs to see a unreasonable amount of scenarios.\\
Insa-Cabrera et al. \cite{cabrera11} used the idea of \cite{orallo10} to construct a general intelligence test for the evaluation of reinforcement learning algorithms. Here, all possible environments are ordered by their Kolmogorov complexity from which a set of samples is drawn. With this set, adaptive tests can be constructed which can be used to evaluate an agent. However, this concept only examines well-known properties of Q-learning. It is not known if this method works for other reinforcement learning algorithms, too. The problem, that all possible environments are needed, still exists.

\subsubsection{Co-Evolution}
\label{coevolutio}
Gabor et al. \cite{gabor18} claim that all steps related to testing adaptive systems need to become self-adaptive. In order to achieve adaptive quality assurance, they introduced the paradigm of scenario coevolution. Instead of randomly creating test scenarios, this architecture describes a set of test cases, that evolves in parallel to the behavior of the system. In this publication, the co-evolutionary process is applied to software engineering, where the test wants to find bad behavior in the productive code, whereas the productive code tries to avoid making mistakes.\\
The testing algorithm and the productive code competitively co-evolve. Wang et al. \cite{wang19} used this concept to generate increasingly complex and diverse learning environments through evolutionary strategies. They train a neural network controlling a walker that tries to optimize for an environment. These environments can be modified by the algorithm (POET - \textbf{P}aired \textbf{O}pen-\textbf{E}nded \textbf{T}railblazer) to create increasingly complex scenarios, the agent needs to solve. This way, POET managed to produce a diverse range of highly developed behaviors for a big range of environments. Unfortunately, this concept can only be used to train the agent. The testing problem still exists.\\
Co-evolutionary approaches in training help the agent to minimize failures. Therefore, it may seem obvious, that co-evolutionary techniques can be used for an efficient evaluation of reinforcement learning systems.