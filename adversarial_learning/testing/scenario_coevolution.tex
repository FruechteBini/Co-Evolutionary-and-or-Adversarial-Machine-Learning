Scenario Co-Evolution method by Gabor et al. \cite{gabor19} is another novel approach to tackle the issue of testing learning systems reliably. If not stated otherwise the following chapter derives from the original publication.\\
The basic idea is to train an evolutionary algorithm, that co-evolves with a reinforcement learning agent and actively finds problems that are hard to solve for the agent. While the expertise of the agent evolves, the set of test instances (or $scenarios$) co-evolves. After training, the co-evolutionary algorithm can return a set of hard test scenarios, that can be used for evaluating the agent. When the agent can solve these problems, it can be assumed that it can solve easy scenarios, too.\\\\
More specifically, hard settings $x$ from the scenario space $\chi$, for which the agent's performance deteriorates are wanted. To find the respective values for $x$, an evolutionary algorithm, that optimizes for hard settings for $x$ is used. It constructs an evolutionary process with  scenario space $\chi$. The experience samples necessary to train the agent are drawn using settings for $x \in \chi$ that are included int the current population $\chi$ of the evolutionary process. When all $x \in X$ have been used, the evolutionary process evolves further for a few generations. The resulting population after a few generations of optimizing for hard $x$ is used to generate experience samples for the reinforcement learning agent. While the reinforcement learning agent tries to maximize its reward, the evolutionary process tries to minimize the fitness. The fitness $f(x)$ assigned to each $x \in \chi$ is computed using the accumulated reward of running the current agent policy $\pi$ on the MDP $M_x$. %Formel?! + Bild Vorgang?! + Umschreiben!
In turn, the reinforcement learning agent with the currently highest fitness score is used to evaluate the hardness of the settings for $x$ for the next few generations of evolution. 