@article{zachary1977information,
  title={An information flow model for conflict and fission in small groups},
  author={Zachary, Wayne W},
  journal={Journal of anthropological research},
  volume={33},
  number={4},
  pages={452--473},
  year={1977},
  publisher={University of New Mexico}
}

@article{berenos10,
    author = {Bérénos, Camillo and Wegner, Mathias and Schmid-Hempel, Paul},
    year = {2011},
    month = {01},
    pages = {218-24},
    title = {Antagonistic coevolution with parasites maintains host genetic diversity: An experimental test},
    volume = {278},
    journal = {Proceedings. Biological sciences / The Royal Society},
    doi = {10.1098/rspb.2010.1211}
}

@article{uesato18,
    author    = {Uesato, Jonathan and Kumar, Ananya and Szepesv{\'{a}}ri, Csaba and Erez, Tom and Ruderman, Avraham and Anderson, Keith and Dvijotham, Krishnamurthy and Heess, Nicolas and Kohli, Pushmeet},
    title     = {Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures},
    journal   = {CoRR},
    volume    = {abs/1812.01647},
    year      = {2018}
}

@article{sallab17,
    author    = {Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil},
    title     = {Deep Reinforcement Learning framework for Autonomous Driving},
    journal =   {Autonomous Vehicles and Machines 2017},
    volume  =   {33},
    number  =   {4},
    pages   =   {70--76},
    year    =   {2017},
    publisher=  {Society for Imaging Science and Technology}
}

@inproceedings{peng04,
    title = {Incremental Multi-Step Q-Learning},
    editor = {William W. Cohen and Haym Hirsh},
    booktitle = {Machine Learning Proceedings 1994},
    publisher = {Morgan Kaufmann},
    address = {San Francisco (CA)},
    pages = {226 - 232},
    year = {1994},
    isbn = {978-1-55860-335-6},
    doi = {https://doi.org/10.1016/B978-1-55860-335-6.50035-0},
    url = {http://www.sciencedirect.com/science/article/pii/B9781558603356500350},
    author = {Peng, Jing and Williams, Ronald J.},
    abstract = {This paper presents a novel incremental algorithm that combines Q-learning, a well-known dynamic programming-based reinforcement learning method, with the TD(A) return estimation process, which is typically used in actor-critic learning, another well-known dynamic programming-based reinforcement learning method. The parameter A is used to distribute credit throughout sequences of actions, leading to faster learning and also helping to alleviate the non-Markovian effect of coarse state-space quantization. The resulting algorithm, Q(λ)-learning, thus combines some of the best features of the Q-learning and actor-critic learning paradigms. The behavior of this algorithm is demonstrated through computer simulations of the standard benchmark control problem of learning to balance a pole on a cart.}
}

@INBOOK{schaal04, 
    author={Schaal, Stefan and Ijspeert, Auke Jan and Billard, Aude and Vijayakumar, Sethu and Meyer, Jean-Arcady}, 
    booktitle={From animals to animats 8: Proceedings of the Eighth International Conference on the Simulation of Adaptive Behavior}, 
    title={Competitive-Cooperative-Concurrent Reinforcement Learning with Importance Sampling}, 
    year={2004}, 
    volume={}, 
    number={}, 
    pages={}, 
    keywords={}, 
    doi={}, 
    ISSN={}, 
    publisher={MITP}, 
    isbn={9780262291446}, 
    url={https://ieeexplore.ieee.org/document/6281993},
}

@Inbook{cruz,
    author={da Cruz, André Abs and Vellasco, Marley Maria Bernardes Rebuzzi and Pacheco, Marco Aurelio},
    editor={Abraham, Ajith and Grosan, Crina and Ishibuchi, Hisao},
    title={Quantum-Inspired Evolutionary Algorithm for Numerical Optimization",
    bookTitle="Hybrid Evolutionary Algorithms},
    year={2007},
    publisher={Springer Berlin Heidelberg},
    address={Berlin, Heidelberg},
    pages={19--37},
    abstract={Since they were proposed as an optimization method, evolutionary algorithms (EA) have been used to solve problems in several research fields. This success is due, besides other things, to the fact that these algorithms do not require previous information regarding the problem to be optimized and offer a high degree of parallelism. However, some problems are computationally intensive regarding the evaluation of each solution, which makes the optimization by EA's slow in some situations. This chapter proposes a novel EA for numerical optimization inspired by the multiple universes principle of quantum computing that presents faster convergence time for the benchmark problems. Results show that this algorithm can find better solutions, with less evaluations, when compared with similar algorithms, which greatly reduces the convergence time.},
    isbn={978-3-540-73297-6},
    doi={10.1007/978-3-540-73297-6_2},
    url={https://doi.org/10.1007/978-3-540-73297-6_2}
}


@article{chen18,
    author    = {Chen, Jie and Ma, Tengfei and Xiao, Cao},
    title     = {FastGCN: Fast Learning with Graph Convolutional Networks via Importance
               Sampling},
    journal   = {CoRR},
    volume    = {abs/1801.10247},
    year      = {2018},
    url       = {http://arxiv.org/abs/1801.10247},
    archivePrefix = {arXiv},
    biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-10247},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{you17,
  author    = {You, Yurong and Pan, Xinlei and Wang, Ziyan and Lu, Cewu},
  title     = {Virtual to Real Reinforcement Learning for Autonomous Driving},
  journal   = {CoRR},
  volume    = {abs/1704.03952},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.03952},
  biburl    = {https://dblp.org/rec/bib/journals/corr/YouPWL17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{strehl09,
    author = {Strehl, Alexander L. and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L.},
    title = {PAC Model-free Reinforcement Learning},
    booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
    series = {ICML '06},
    year = {2006},
    isbn = {1-59593-383-2},
    location = {Pittsburgh, Pennsylvania, USA},
    pages = {881--888},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/1143844.1143955},
    doi = {10.1145/1143844.1143955},
    acmid = {1143955},
    publisher = {ACM},
    address = {New York, NY, USA}
} 

@book{puterman94,
    author = {Puterman, Martin L.},
    title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
    year = {1994},
    isbn = {0471619779},
    edition = {1st},
    publisher = {John Wiley \& Sons, Inc.},
    address = {New York, NY, USA},
} 

@InProceedings{michalewicz93,
    author = {Michalewicz, Zbigniew},
    editor = {Yao, Xin},
    title = {A perspective on evolutionary computation},
    booktitle = {Progress in Evolutionary Computation},
    year = {1995},
    publisher = {Springer Berlin Heidelberg},
    address = {Berlin, Heidelberg},
    pages = {73--89},
    abstract = {During the last three decades there has been a growing interest in algorithms which rely on analogies to natural processes. The emergence of massively parallel computers made these algorithms of practical interest. The best known algorithms in this class include evolutionary programming, genetic algorithms, evolution strategies, simulated annealing, classifier systems, and neural networks.},
    isbn = {978-3-540-49528-4}
}



@inproceedings{gabor19,
    author    = {Gabor, Thomas and Sedlmeier, Andreas and Kiermeier, Marie and Phan, Thomy and Henrich, Marcel and Picklmair, Monika and Kempter, Bernhard and Klein, Cornel and Sauer, Host and Schmid, Reiner and Wieghardt, Jan},
    title     = {Scenario Co-Evolution for Reinforcement Learning on a GridWorld Smart Factory Domain},
    booktitle    = {28th Genetic and Evolutionary Computation Conference (GECCO ’19)},
    year      = {2019}
}

@article{gabor18,
    author    = {Gabor, Thomas and Kiermeier, Marie and Sedlmeier, Andreas and Kempter, Berhard and Klein, Cornel and Sauer, Horst and Schmid, Reiner N. and Wieghardt, Jan},
    title     = {Adapting Quality Assurance to Adaptive Systems: The Scenario Coevolution
               Paradigm},
    journal   = {CoRR},
    volume    = {abs/1902.04694},
    year      = {2019},
    url       = {http://arxiv.org/abs/1902.04694},
    archivePrefix = {arXiv},
    eprint    = {1902.04694},
    timestamp = {Tue, 21 May 2019 18:03:40 +0200},
    biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-04694},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wang19,
    author    = {Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O. },
    title     = {Paired Open-Ended Trailblazer {(POET):} Endlessly Generating Increasingly
               Complex and Diverse Learning Environments and Their Solutions},
    journal   = {CoRR},
    volume    = {abs/1901.01753},
    year      = {2019},
    url       = {http://arxiv.org/abs/1901.01753},
    archivePrefix = {arXiv},
    eprint    = {1901.01753},
    timestamp = {Thu, 31 Jan 2019 13:52:49 +0100},
    biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-01753},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{orallo10,
    author = {Hernandez-Orallo, Jose and L. Dowe, David},
    year = {2010},
    month = {12},
    pages = {1508-1539},
    title = {Measuring Universal Intelligence: Towards an Anytime Intelligence Test},
    volume = {174},
    journal = {Artificial Intelligence},
    doi = {10.1016/j.artint.2010.09.006}
}

@InProceedings{cabrera11,
    author  =  {Insa-Cabrera, Javier and Dowe, David L. and Hern{\'a}ndez-Orallo, Jos{\'e}},
    editor ={Lozano, Jose A. and G{\'a}mez, Jos{\'e} A. and Moreno, Jos{\'e} A.},
    title   =  {Evaluating a Reinforcement Learning Algorithm with a General Intelligence Test},
    booktitle =  {Advances in Artificial Intelligence},
    year    =  {2011},
    publisher =  {Springer Berlin Heidelberg},
    address =  {Berlin, Heidelberg},
    pages   =  {1--11},
    abstract=  {In this paper we apply the recent notion of anytime universal intelligence tests to the evaluation of a popular reinforcement learning algorithm, Q-learning. We show that a general approach to intelligence evaluation of AI algorithms is feasible. This top-down (theory-derived) approach is based on a generation of environments under a Solomonoff universal distribution instead of using a pre-defined set of specific tasks, such as mazes, problem repositories, etc. This first application of a general intelligence test to a reinforcement learning algorithm brings us to the issue of task-specific vs. general AI agents. This, in turn, suggests new avenues for AI agent evaluation and AI competitions, and also conveys some further insights about the performance of specific algorithms.},
    isbn    =  {978-3-642-25274-7}
}



@article{MachineLearningAtScale,
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	title = {{Adversarial Machine Learning at Scale}},
	journal = {arXiv},
	year = {2016},
	month = {Nov},
	eprint = {1611.01236},
	url = {https://arxiv.org/abs/1611.01236}
}

@article{harnessing_goodfellow,
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	title = {{Explaining and Harnessing Adversarial Examples}},
	journal = {arXiv},
	year = {2014},
	month = {Dec},
	eprint = {1412.6572},
	url = {https://arxiv.org/abs/1412.6572}
}

@article{brundage17, 
    author={K. {Arulkumaran} and M. P. {Deisenroth} and M. {Brundage} and A. A. {Bharath}}, 
    journal={IEEE Signal Processing Magazine}, 
    title={Deep Reinforcement Learning: A Brief Survey}, 
    year={2017}, 
    volume={34}, 
    number={6}, 
    pages={26-38}, 
    keywords={cameras;learning (artificial intelligence);mobile robots;neural nets;deep reinforcement learning;artificial intelligence;autonomous systems;robot control policies;camera inputs;value-based method;policy-based method;central algorithms;deep Q-network;trust region policy optimization;asynchronous advantage actor critic;deep neural networks;Artificial intelligence;Signal processing algorithms;Visualization;Machine learning;Learning (artificial intelligence);Neural networks}, 
doi={10.1109/MSP.2017.2743240}, 
ISSN={1053-5888}, 
month={Nov},}

@article{selfplay-heinrich,
	author = {Heinrich, Johannes and Silver, David},
	title = {{Deep Reinforcement Learning from Self-Play in Imperfect-Information Games}},
	journal = {arXiv},
	year = {2016},
	month = {Mar},
	eprint = {1603.01121},
	url = {https://arxiv.org/abs/1603.01121}
}

@article{environmentBansal2017Oct,
	author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
	title = {{Emergent Complexity via Multi-Agent Competition}},
	journal = {arXiv},
	year = {2017},
	month = {Oct},
	eprint = {1710.03748},
	url = {https://arxiv.org/abs/1710.03748}
}

@article{robustPinto2017Mar,
	author = {Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
	title = {{Robust Adversarial Reinforcement Learning}},
	journal = {arXiv},
	year = {2017},
	month = {Mar},
	eprint = {1703.02702},
	url = {https://arxiv.org/abs/1703.02702}
}

@techreport{multiuther1997adversarial,
  title={Adversarial reinforcement learning},
  author={Uther, William and Veloso, Manuela},
  year={1997},
  institution={Technical report, Carnegie Mellon University, 1997. Unpublished}
}

@inproceedings{multiuther1997generalizing,
  title={Generalizing adversarial reinforcement learning},
  author={Uther, William TB and Veloso, Manuela M},
  booktitle={Proceedings of the AAAI Fall Symposium on Model Directed Autonomous Systems},
  volume={206},
  year={1997},
  organization={Citeseer}
}

@inproceedings{boyan1995generalization,
  title={Generalization in reinforcement learning: Safely approximating the value function},
  author={Boyan, Justin A and Moore, Andrew W},
  booktitle={Advances in neural information processing systems},
  pages={369--376},
  year={1995}
}

@article{tetrisRovatsou2010May,
	author = {Rovatsou, Maria and Lagoudakis, Michail G.},
	title = {{Minimax Search and Reinforcement Learning for Adversarial Tetris}},
	journal = {SpringerLink},
	pages = {417--422},
	year = {2010},
	month = {May},
	publisher = {Springer, Berlin, Heidelberg},
	doi = {10.1007/978-3-642-12842-4_53}
}

@article{animationwampler2010character,
  title={Character animation in two-player adversarial games},
  author={Wampler, Kevin and Andersen, Erik and Herbst, Evan and Lee, Yongjoon and Popovi{\'c}, Zoran},
  journal={ACM Transactions on Graphics (TOG)},
  volume={29},
  number={3},
  pages={26},
  year={2010},
  publisher={ACM}
}
@article{astroGANSchawinski2017Feb,
	author = {Schawinski, Kevin and Zhang, Ce and Zhang, Hantian and Fowler, Lucas and Santhanam, Gokula Krishnan},
	title = {{Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution limit}},
	journal = {arXiv},
	year = {2017},
	month = {Feb},
	eprint = {1702.00403},
	doi = {10.1093/mnrasl/slx008}
}

@inproceedings{styletransferhuang2017arbitrary,
  title={Arbitrary style transfer in real-time with adaptive instance normalization},
  author={Huang, Xun and Belongie, Serge},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1501--1510},
  year={2017}
}

@article{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  publisher={King's College, Cambridge}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@article{baker2016designing,
  title={Designing neural network architectures using reinforcement learning},
  author={Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
  journal={arXiv preprint arXiv:1611.02167},
  year={2016}
}

@article{roboticstone1998towards,
  title={Towards collaborative and adversarial learning: A case study in robotic soccer},
  author={Stone, Peter and Veloso, Manuela},
  journal={International Journal of Human-Computer Studies},
  volume={48},
  number={1},
  pages={83--104},
  year={1998},
  publisher={Elsevier}
}

@article{GoalphaGosilver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{chessSilver2017mastering,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@article{montecarlobrowne2012survey,
  title={A survey of monte carlo tree search methods},
  author={Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
  publisher={IEEE}
}

@misc{dotaOpenAI2019Jun,
	author = {OpenAI},
	title = {{OpenAI Five}},
	journal = {OpenAI},
	year = {2019},
	month = {Jun},
	publisher = {OpenAI},
	note = {[Online; accessed 16. Jun. 2019]},
	url = {https://openai.com/five}
}

@article{ppoSchulman2019Mar,
	author = {Schulman, John},
	title = {{Proximal Policy Optimization}},
	journal = {OpenAI},
	year = {2019},
	month = {Mar},
	publisher = {OpenAI},
    note = {[Online; accessed 16. Jun. 2019]}
	url = {https://openai.com/blog/openai-baselines-ppo}
}

@inproceedings{TDGammontesauro1992practical,
  title={Practical issues in temporal difference learning},
  author={Tesauro, Gerald},
  booktitle={Advances in neural information processing systems},
  pages={259--266},
  year={1992}
}

@article{TD2Gammontesauro1995temporal,
  title={Temporal difference learning and TD-Gammon},
  author={Tesauro, Gerald},
  journal={Communications of the ACM},
  volume={38},
  number={3},
  pages={58--68},
  year={1995}
}

@article{lavet18,
  author    = {Fran{\c{c}}ois{-}Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  title     = {An Introduction to Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1811.12560},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.12560},
  eprint    = {1811.12560},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-12560},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{basics1littman1994markov,
  title={Markov games as a framework for multi-agent reinforcement learning},
  author={Littman, Michael L},
  booktitle={Machine learning proceedings 1994},
  pages={157--163},
  year={1994},
  publisher={Elsevier}
}

@inproceedings{basics2hu1998multiagent,
  title={Multiagent reinforcement learning: theoretical framework and an algorithm.},
  author={Hu, Junling and Wellman, Michael P and others},
  booktitle={ICML},
  volume={98},
  pages={242--250},
  year={1998},
  organization={Citeseer}
}

@article{nashgharesifard2013distributed,
  title={Distributed convergence to Nash equilibria in two-network zero-sum games},
  author={Gharesifard, Bahman and Cort{\'e}s, Jorge},
  journal={Automatica},
  volume={49},
  number={6},
  pages={1683--1692},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{gansgoodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@article{nvidiaThisPersonDoesNotExistKarras2018Dec,
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	title = {{A Style-Based Generator Architecture for Generative Adversarial Networks}},
	journal = {arXiv},
	year = {2018},
	month = {Dec},
	eprint = {1812.04948},
	url = {https://arxiv.org/abs/1812.04948}
}

@article{resolutionGANWang2018Sep,
	author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Loy, Chen Change and Qiao, Yu and Tang, Xiaoou},
	title = {{ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks}},
	journal = {arXiv},
	year = {2018},
	month = {Sep},
	eprint = {1809.00219},
	url = {https://arxiv.org/abs/1809.00219v2}
}

@article{awad2018moral,
  title={The moral machine experiment},
  author={Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz, Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon, Jean-Fran{\c{c}}ois and Rahwan, Iyad},
  journal={Nature},
  volume={563},
  number={7729},
  pages={59},
  year={2018},
  publisher={Nature Publishing Group}
}
