\section{Introduction}
\label{Introduction}

One of the central objectives in the area of artificial intelligence (AI) is to create fully autonomous agents. These systems should improve over time through trial and error, so the trained agents will eventually know the optimal behaviour when interacting with their environment \cite{brundage17}. Reinforcement learning (RL) is a principled mathematical framework to achieve experience-based autonomous learning \cite{strehl09}.\\
In recent years, RL methods have driven impressive advances in AI, achieving superhuman performance in numerous domains \cite{chessSilver2017mastering, GoalphaGosilver2017mastering}. Several of these advances have been achieved by combining RL with deep learning techniques. Especially for problems with high dimensional state-space, this combination has proven to be powerful \cite{lavet18}.
Despite revolutionary progress, limitations still exists due to memory complexity, computational complexity and sample complexity \cite{strehl09}. Therefore, some applications require a level of knowledge that surpasses the capabilities achieved by AI.\\
Autonomous driving is one example \cite{sallab17}. The car has to interact strongly with its complex environment (e.g. other vehicles, pedestrians, roadworks) and will only be brought on the market if it is near to risk free. The straight-forward idea of solving these problems would be end-to-end supervised learning. This approach usually demands massive amount of labelled data and consequently requires significant (time consuming) human involvement. By contrast, RL typically learns by trial-and-error and does not need explicit human supervision \cite{you17}. Still, due to the high complexity and safety issues of autonomous driving, training and testing with standard RL trial-and-error methods may be prohibitively expensive and time consuming \cite{uesato18}.\\
To enhance RL algorithms, biologically influenced extensions of the basic concept have emerged from recent studies \cite{environmentBansal2017Oct, gabor19, robustPinto2017Mar, uesato18}. Some inspiration derived from predator-prey relationships, which are often observed in biology. Both evolutionary processes try to influence each others fitness (co-evolution). The predator tries to maximize its reward by eating the prey, while the prey wants to achieve the exact opposite. Theoretically, when antagonistic populations are trying to outperform each other, it may result in fast improvements and increased genetic robustness \cite{berenos10}. This natural phenomenon is called competitive co-evolution and can be transferred to evolutionary algorithms \cite{gabor19}.\\
In this paper, we exclusively focus on concepts of competitive co-evolutionary and adversarial learning. First, we introduce the basic concepts (chapter \ref{Basics}) needed to understand said concepts. Then, we present and compare different state of the art adversarial and co-evolutionary algorithms for training (chapter \ref{adv_training}) and testing (chapter \ref{testing}) reinforcement learning agents.