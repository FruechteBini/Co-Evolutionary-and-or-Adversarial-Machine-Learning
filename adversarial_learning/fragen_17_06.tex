- multiagent systems allgemein (als einleitung für adversarial setting)

--> dann speziell auf settings mit zero-sum game eingehen
- self - play als homogenes zero sum game
- gans als heterogenes zero sum game

--> dann: vergleich: adv multi agent <-> zero sum gameqe 


- Basics?
was rein?
	- reinforcement learning? - auf jeden fall
    - deep neural networks? - deep reinforcement learning (reinforcement mit neuronalen netzen) (q-learning)
    - value network vs policy network - nein
    - monte-carlo-search-trees - nein
    - PPO (proximal policy optimization) -nein
    - zero sum games (!) - definition - konsistente notation 
    
    - Formeln (markov decision processes)
    

- Multi-Agent competition (nicht unbedingt zero sum game)
	- name okay?
    - algorithmen wie weit erklären? ((fitted)prioritized sweeping, u-tree, q-learning, minmax usw.)
    - bisher:
    	- RARL
        - tetris
        - realistic movement in computer games ("fangen")
        - robotic

- GANs als eigenen Punkt oder großer Unterpunkt von MultiAgent? mittelpunkt / gleichgewicht nash-equilibrium (asymmetrisch im vergleich zu self-play)

- Self-play (homogene)

bisher:
	- TD-Gammon
    - AlphaGo (zero)
    - OpenAI Dota 2
    - OpenAI physical self-play (simulated)
    
    FRAGE: WIE genau funktioniert self-play? (reinforcement learning, rewards, etc.)
    
anderes:
	- OpenAI website als quelle
    - OpenAI youtube video als quelle (rede von mitarbeiter)
    - Einleitung: Vergleich mit Menschen, Sport
    
    
    
    
    // treffen 3.7.
    
    TDGammon --> AlphaZero --> OpenAI Five
    
    (was hat sich geändert?=
    
    
    AlphaGo - modelbased  (es gibt regeln) (montecarlo tree search)
    
    https://news.ycombinator.com/item?id=15627877
    https://www.quora.com/What-is-the-difference-between-model-based-and-model-free-reinforcement-learning
    
    temporal difference learning
    
    OpenAi Five --> modelfree
    
    GANS bei Go -> zwei netze --> mehr strom als 1
    
    