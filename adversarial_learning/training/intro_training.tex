\begin{comment}
- MOTIVATION
- limitations of random testing --> adversarial approach
- gap between simulation & real world
- model uncert
- adversarial learning is a system which involves focussing on the systems weaknesness
\end{comment}
\begin{comment}

- computer performance, reinforcement learning gets better
- traditional RL fails in real world --> gap between simulation & real world big & not enough training data (set)
- when prepared for real world -> agent should be trained with every potential danger/disturbance to the system
- training sets are limited
- new approach: adversarial reinforcement learning 

//Definition:
-Adversarial training: training the model to malicious input on purpose -> make it more robust to attack, unknown input and reduce test error \cite{MachineLearningAtScale}
- adversarial inputs are "blind spots" in training set \cite{harnessing_goodfellow}
- Linear nature of machine learning is the cause of this \cite{harnessing_goodfellow}
- Agent learns to operate in the presence of a destabilizing adversary -> the adversary agent learns an optimal destabilizing policy --> becomes the "perfect" enemy and generates training data better & broader than a human generated one

-

\end{comment}

Many reinforcement-based approaches tend to fail in real life since the gap between simulation and real-world implementation is very large. If an agent is to be trained for the real world, he should be prepared for every possible external influence and danger, because the algorithm could have responsibility for human lives. According to Pinto et al. \cite{robustPinto2017Mar} there are two ways to perform policy learning for physical tasks: Training in real-world which is expensive, dangerous and time-intensive. Not a good option. The second method is more promising, that is to simulate the tasks and later transfer the learned policy into the real world. But also here the environment is often not identical to the real world and therefore the gap is still not closed. In addition training sets taken from human experience are often limited and overlook such adversarial inputs, causing overfitting \cite{robustPinto2017Mar}. According to Goodfellow et al. \cite{harnessing_goodfellow}, the reason for this behavior is the linear nature of neural networks and adversarial inputs are "blind spots" in the training set \inline{nochmal nachschauen}

The aim of the Adversarial Learning approach is to train the model intentionally with critical input. The goal is to make the trained agent more robust against attacks and unknown inputs. In addition, the test error should be minimized \cite{MachineLearningAtScale}.
The complexity of an agent strongly depends on the complexity of the environment in which the agent operates. The greater and more difficult the influences of the environment, the more complex it becomes for the agent to achieve his goal \cite{environmentBansal2017Oct}.
In order to make the environment more arduous, a destabilizing unit is set against the agent. The enemy learns an optimal destabilizing policy and thus becomes the "perfect" opponent \cite{robustPinto2017Mar}. Therefore, the adversary automatically generates a challenging, broad training set that could probably not be produced by humans.
There are different approaches of adversarial machine learning. In the following we will observe and compare three types. First we have a look at a general overview of what is considered to be adversarial machine learning in a multi-agent setting in \ref{multiagent}. Next, we look at "Self-Play", where one agent competes against a copy of itmself (\ref{selfplay}). Finally, in \label{gans} we take a look at "General Adversarial Networks", where a generator and a discriminator work against each other in an adversarial setup \ref{zerosumgames}.

At the end of this chapter we will compare previously discussed scenarios self-play and GANs. In the comparison we will refer to zero-sum games (\ref{zerosumgames}. A zero-sum game in self-play behaves homogeneously, in GANs we consider a heterogeneous setup. 