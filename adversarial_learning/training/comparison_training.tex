 \subsection{Comparison}
\label{adv_comparison}
 
 \begin{comment}
 
- vergleichen, wie es sich nach jeder trainingsrunde verhält: bei self-play wird der bessere genommen - bei GANs muss zb diskriminator "warten" ? 
 
 - algorithmen vergleichen - entscheidung, update, 
 
 - homogenes gameplay bei Go: beide agents haben gleiche aufgabe.
 
 - heterogenes gmaeplay bei GANs: Spieler verfolgen genau entgegengesetzte Ziele.
 
 
 - GANs sind wohl der allgemeinere Fall - aber warum sollte man dann Self-Play verwenden?
- Bietet ein Ansatz (theoretische) Garantieen, die ein anderer Ansatz nicht hat? (Garantie, dass der Gegner immer gleich gut ist beim self-play zb) 
- Was ist effizienter und leichter zu implementieren? 
- Gibt es Beispiele für Agentensysteme oder praktische Anwendungsfälle, in denen das eine mehr Sinn macht als das andere (abgesehen vom offensichtlichen Fall homogen vs. heterogen).
 
 
\end{comment} 
 
 
 To close the training chapter, self-play (\ref{selfplay}) and GANs (\ref{gans}) will now be compared.
 For our analysis we consider the zero-sum game (\ref{zerosumgames}) as a setting. 
 %The comparison is interesting because the self-play operates as a homogeneous zero-sum game, while GANs are a heterogeneous entity. This means that in self-play both agents logically follow the same goal, while in GANs opposite goals are pursued.\\

%GANs sind wohl der allgemeinere Fall - aber warum sollte man dann Self-Play verwenden?

\subsubsection{When to use which approach?}
%own experience
%no supervision of data
%no dataset

The first question we will deal with is: When is what type of adversarial learning applied?
Of course, what is decisive here is which goal is pursued and, above all, in which environment the agent is to be trained.\\

%GAN's general case:
GANs in our case represent the classical approach (we refer to it as the classical approach), where the primary goal is to train an agent and to make the training constantly more difficult by deploying an opposite agent. We call this type of adversarial training, in which rewards are given for opposite goals, heterogeneous. For instance the examples mentioned in \ref{multiagent}.
%This is to increase the complexity of the desired agent \ref{multiagent}. One must mention, however, that the opponent of the trained algorithm was at the end only a helpful means to the end


%When self-play?
Zero-sum games in which two players pursue the same goal and have the same environment and resources are particularly suitable for self-play. We're talking about homogeneous adversarial training here. Examples would be AlphaGo Zero, both agents have one playing field, the same number of stones and one goal: biggest territory at the end. Equivalent to this are for example Dota 2 (one map, destruction of the opponent's fortress) or Chess (a board, checkmate as target).
In principle, all applications implemented as self-play could also be implemented as a classical adversarial approach. In Go, for example, two neural networks could play against each other, but not learn in a coordinated way. On the other hand, not every classical adversarial training could also take place as self-play. So the idea of implementing two generators in one GAN would make little sense. A discriminator would still be needed to evaluate the generated data.

Translated with www.DeepL.com/Translator

\subsubsection{What do the approaches promise /what are the drawbacks?}


\subsubsection{What do the }