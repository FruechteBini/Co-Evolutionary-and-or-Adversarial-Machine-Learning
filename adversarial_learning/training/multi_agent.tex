\subsection{Multi-Agent Competition}
\label{multiagent}
%\inline{hier predator-prey wieder aufnehmen}
Multi-Agent Competition is primarily about training multiple agents as well as possible by means of reinforcement learning. For simplification, we always refer to a two-player scenario in this work.\\
\\
To illustrate this, we take up again the example of predator and prey (mentioned in section \ref{Introduction}), which can be observed in biology.
The main goal is to train a main agent, the prey, robust against all types of inconvenience.
To achieve this, a predator is applied who learns an optimal policy also via reinforcement learning to maximize the difficulty for the prey to gain a high score \cite{robustPinto2017Mar}. As a result, both prey and predator evolve and are forced to explore new possibilities and scenarios to maximize their reward. This leads to an increasingly complex environment that prepare the agent for what may be unpredictable for humans.

\subsubsection{Applications of multi-agent adversarial learning}
\label{appl_train}
%\inline{Veranschaulichung von adversarial settings}

To help to understand adversarial learning we will look at some application areas and examples where it has been used for training in a multi-agent environment.\\
\\
Pinto et al \cite{robustPinto2017Mar} have conducted extensive experiments with simulated agents who are supposed to learn physical tasks such as running, jumping, swimming or balancing. They call their approach RARL (robust adversarial reinforcement learning). Agents who perform classical reinforcement learning would limit themselves to elementary solutions to get their reward. In order to make the conditions more difficult and thus to make the trained agent robust, a adversary was deployed. The task of the adversary is to prevent the target of the trained agent from being reached. The adversary is able to manipulate parameters that define the agent, such as mass, friction, etc. and also modify the environment by letting it e.g. walk on ice instead of the usual ground.
The result of their investigations was that an agent trained using RARL is more robust to environmental changes than classic reinforcement learning. In addition, the learned policy performs better with random initialization of the parameters. This in turn means that the training set can be more diversified because the agent is less sensitive. The framework was also more robust in the test environment against disturbances that are difficult to design during the training \cite{robustPinto2017Mar}.\\
\\
"Adversarial Tetris" is another example of adversarial machine learning \cite{tetrisRovatsou2010May}. The task of the primary agent is to handle as many lines as possible. In contrast to the classic Tetris, the tetraminoes are not randomly selected, but are chosen as inappropriately as possible by an opposing agent. Additionally the adversary and the size of the playing field varies. 
%\inline{minmax mit alpha beta pruning}
The agent showed satisfactory results against many different combinations of opponents and fields. It also did well in subsequent test runs \cite{tetrisRovatsou2010May}.\\
\\
Competitive computer games are becoming more and more realistic. The animation of character movements plays an important role. In order to make movements look "realistic", randomness is indispensable. Wampler et al. \cite{animationwampler2010character} used adversarial machine learning to simulate intelligent, forward-thinking, unpredictable movements of a character in a game. 
For this purpose, agents competing against each other were simulated, for instance the game "Tap". An agent tries to touch another agent with its hand. The escaping opponent develops a policy, which has evasion as a goal.
The movements were simulated and show that the agents evolve human-like movements. For example, the evasive character takes many quick, small steps as soon as the catcher approaches. This gives him the opportunity to be unpredictable and to dodge to the right or left at any time.
The agents also learned to fake movements and guessed the possible strategies of the respective opponent \cite{animationwampler2010character}. \\
\\
This kind of learning is also applicable to robotics. For example, robots that play football. First, robots have to run, aim, learn to shoot. However, as soon as a defender comes into play, Stone et al. \cite{roboticstone1998towards} suggested adversarial reinforcement learning could be used to improve both attackers and defenders. The defender would have the opportunity to learn to recognize in which direction the attacker is aiming. The attacker could at the same time adjust his shot path based on the defender's movements. 
Defender and attacker would thus co-develop and challenge each other with an adaptive training set \cite{roboticstone1998towards}.

\subsubsection{State explosion in Multi-Agent environments}

One problem faced when adding additional agents is that the state number grows exponentially. Learning speed is negatively affected and in many states the precise position of the opponent is not relevant \cite{multiuther1997generalizing}.
Describing such a large set of states individually using a value function is intractable, so a form of generalization must be used \cite{boyan1995generalization}. 
To avoid this exponential mass of states, Uther et al. \cite{multiuther1997generalizing} compared different non-generalizing and generalizing algorithms in a multi-agent environment. 
Their application case was Hexcer - a two-player hexagonal grid soccer game. Two agents are the adversary to each other and try to score/ prevent the enemy from scoring. Their findings showed that both their generalizing algorithms (Continuous U Tree, Fitted Prioritized Sweeping) performed significantly better than the non-generalizing algorithms (Prioritized Sweeping) \cite{multiuther1997generalizing}.
