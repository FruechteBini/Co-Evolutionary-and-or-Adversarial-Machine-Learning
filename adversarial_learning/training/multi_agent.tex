\subsection{Multi-Agent Competition}
%phan paper training 2

Multi-Agent Competition is primarily about training an agent 1 as well as possible by means of reinforcement learning. The goal is to train him robust against all types of inconvenience.
%and to close the gap between simulation and the real world as good as possible.
%anders formulieren
To achieve this an agent 2 is applied who learns an optimal policy also via reinforcement learning to make agent A's life as difficult as possible \cite{robustPinto2017Mar}.

\subsubsection{Related work}

To help understand adversarial learning we will look at some application areas and examples where it has been used for training in a multi-agent environment.\\

Pinto et al \cite{robustPinto2017Mar} have conducted extensive experiments with simulated agents who are supposed to learn physical tasks such as running, jumping, swimming or balance. They call their approach RARL (robust adversarial reinforcement learning). Agents who perform classical reinforcement learning would limit themselves to elementary solutions to get their reward. In order to make the conditions more difficult and thus to make agent 1 robust, agent 2 was deployed. The task of this agent is to prevent the target of agent 1 from being reached. Agent 2 is able to manipulate parameters that define agent 1, such as mass, friction, etc. and also modify the environment by letting him e.g. walk on ice instead of the usual ground.
The result of their investigations was that RARL is more robust to environmental changes than classical reinforcement learning. In addition, the learned policy performs better with random initialization of the parameters. This in turn means that the training set can be more diversified because the agent is less sensitive. The framework was also more robust in the test environment against disturbances that are difficult to design during the training \cite{robustPinto2017Mar}.

"Adversarial Tetris" is another good example of adversarial machine learning \cite{tetrisRovatsou2010May}. The task of the primary agent is to handle as many lines as possible. In contrast to the classic Tetris, the tetraminoes are not randomly selected, but are chosen as inappropriately as possible by an opposing agent. Additionally the adversary and the size of the playing field varies. \inline{minmax mit alpha beta pruning}
The agent showed satisfactory results against many different combinations of opponents and fields. He also did well in subsequent test runs \cite{tetrisRovatsou2010May}.

Competitive computer games are becoming more and more realistic. The animation of character movements plays an important role. In order to make movements look "realistic", randomness is indispensable. Wampler et al. \cite{animationwampler2010character} used adversarial machine learning to simulate intelligent, forward-thinking, unpredictable movements of a character in a game. 
For this purpose, agents competing against each other were simulated, among other things to perform was the game "Tap". An agent tries to touch another agent with his hand. The running away opponent develops a policy, which has evasion as a goal.
The movements were simulated and let observe, how the agents evolve human-like movements. For example, the evasive character takes many quick, small steps as soon as the catcher approaches. This gives him the opportunity to be unpredictable and to dodge to the right or left at any time.
The agents also learned to fake movements and guessed the possible strategies of the respective opponent \cite{animationwampler2010character}.

This kind of learning also makes sense in robotics. For example, robots that play football. First, robots have to run, aim, learn to shoot. However, as soon as a defender comes into play, Stone et al. \cite{roboticstone1998towards} said adversarial reinforcement learning would be used to improve both attackers and defenders. The defender would have the opportunity to learn to recognize in which direction the attacker is aiming. The attacker could at the same time adjust his shot path based on the defender's movements. 
Defender and attacker would thus co-develop and challenge each other with an adaptive training set \cite{roboticstone1998towards}.

\subsubsection{State explosion in Multi-Agent environments}

The problem faced when adding additional agents is that it allows the state number to grow exponentially. Learning speed is negatively affected and in many states the precise position of the opponent is not of any relevance \cite{multiuther1997generalizing}.
Describing such a large set of states individually using a value function is intractable, so a form of generalization must be used \cite{boyan1995generalization}. 
To avoid this exponential mass of states, Uther et al. \cite{multiuther1997generalizing} compared different non-generalizing and generalizing algorithms in a multi-agent environment. 
Their application case was Hexcer - a two-player multi-agent hexagonal grid soccer game. Two agents are the adversary to each other and try to score/ prevent the enemy from scoring. Their findings showed that both their generalizing algorithms (Continuous U Tree, Fitted Prioritized Sweeping) performed significantly better than the non-generalizing algorithms (Prioritized Sweeping) \cite{multiuther1997generalizing}.
%hier evtl algorithmen kurz erklären? bzw überhaupt sinnvoll?

%beispiele suchen und beschreiben!